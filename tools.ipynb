{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.utilities import WikipediaAPIWrapper, DuckDuckGoSearchAPIWrapper\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "wikipedia_tool = WikipediaAPIWrapper()\n",
    "web_search_tool = DuckDuckGoSearchAPIWrapper()\n",
    "\n",
    "@tool\n",
    "def buscar_informacion_persona(nombre: str) -> str:\n",
    "    \"\"\"Get information of a specific 'name' given in the web.\"\"\"\n",
    "    try:\n",
    "        info_wikipedia = wikipedia_tool.run(nombre)\n",
    "        if info_wikipedia:\n",
    "            print(\"Información obtenida de Wikipedia:\")\n",
    "            print(info_wikipedia)\n",
    "            return info_wikipedia\n",
    "        else:\n",
    "            print(\"No se encontró información en Wikipedia.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al buscar en Wikipedia: {e}\")\n",
    "\n",
    "    print(\"Intentando buscar en la web...\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "class ExtractPersonalDataArgs(BaseModel):\n",
    "    textos: list\n",
    "    nombre_persona: str\n",
    "\n",
    "@tool\n",
    "def extraer_datos_personales_openai(data):\n",
    "    \"\"\" Based on a json with keys, textos of type List and nombre_persona of type string, extract the news and nombre_persona and get the personal data of a specific person based on the news List  \"\"\"\n",
    "    data = json.loads(data)\n",
    "    textos = data.get('textos')\n",
    "    nombre_persona = data.get('nombre_persona')\n",
    "    \n",
    "    texto_concatenado = \" \".join(textos)\n",
    "    prompt_template = \"\"\"\n",
    "        Extrae información personal sobre {nombre_persona} de este texto y escribe un resumen en forma de párrafo.\n",
    "        Incluye, si están presentes, el nombre completo, fecha de nacimiento, lugar de residencia, datos de contacto, ocupación, logros, relaciones relevantes y cualquier cita textual significativa.\n",
    "        Si no hay información relevante sobre {nombre_persona}, responde 'No se encontró información personal relevante'.\n",
    "        Texto:\n",
    "        {texto_concatenado}\"\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "    formatted_prompt = prompt.format_messages(texto_concatenado=texto_concatenado, nombre_persona=nombre_persona)\n",
    "    response = chat(formatted_prompt).content\n",
    "\n",
    "  \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from difflib import SequenceMatcher\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "def es_similar(nombre1, nombre2, umbral=0.8):\n",
    "    return SequenceMatcher(None, nombre1, nombre2).ratio() >= umbral\n",
    "\n",
    "@tool\n",
    "def filtrar_noticias_por_persona(data):\n",
    "    \"\"\" Get of input a dictionary with keys 'noticias' and 'nombre_persona' and based on that get the news what are related with a specific user. \"\"\"\n",
    "    data = json.loads(data)\n",
    "    noticias = data.get('noticias')\n",
    "    nombre_persona = data.get('nombre_persona')\n",
    "\n",
    "\n",
    "    nlp = spacy.load(\"es_core_news_md\")\n",
    "    nombre_persona = nombre_persona.lower()\n",
    "    noticias_filtradas = []\n",
    "\n",
    "    for noticia in noticias:\n",
    "        doc = nlp(noticia)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PER\" and es_similar(nombre_persona, ent.text.lower()):\n",
    "                noticias_filtradas.append(noticia)\n",
    "                break\n",
    "\n",
    "    return noticias_filtradas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def contar_personas_mencionadas(data):\n",
    "    \"\"\"Get a dict with key 'noticias', and return the count of persons in the given list of 'noticias'. \"\"\"\n",
    "    data = json.loads(data)\n",
    "    noticias = data.get('noticias')\n",
    "    personas = []\n",
    "    for noticia in noticias:\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"texto\"],\n",
    "            template=\"Extrae todos los nombres de personas mencionadas en el siguiente texto: {texto}\"\n",
    "        )\n",
    "        llm_chain = LLMChain(llm=OpenAI(), prompt=prompt)\n",
    "        personas_encontradas = llm_chain.run(noticia)  \n",
    "        personas.extend(personas_encontradas.split(\", \"))\n",
    "    return len(set(personas))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def obtener_datos_consulta(texto):\n",
    "    \"\"\"Given a text return a dictionary with the topic of the news, and the start_date and the end_date. \"\"\"\n",
    "    template = \"\"\"\n",
    "    Extrae la temática, la fecha de inicio y la fecha de fin del siguiente texto. \n",
    "            El formato de la fecha, tanto inicio como fin debe ser el siguiente: Y-m-d. \n",
    "                Devuelve los resultados en formato JSON con las claves: tematica, fecha_inicio, fecha_fin\n",
    "                en caso de no existir alguno de los valores, deja los campos como vacios.\n",
    "                Texto: {texto}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    formatted_prompt = prompt.format_messages(texto=texto, pregunta=\"¿Qué es el aprendizaje profundo?\")\n",
    "    response = chat(formatted_prompt).content\n",
    "    response_dict = json.loads(response)\n",
    "    return response_dict\n",
    "\n",
    "# datos = obtener_datos_consulta(\"Cuales son las personas que más se mencionan entre dos fechas sobre la temática del hidrogeno verde? \")\n",
    "# chroma_db_data = buscar_en_chromadb(datos)\n",
    "# print(chroma_db_data)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
